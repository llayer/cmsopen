{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bound-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.20/06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eos/home-l/llayer/cmsopen/columnar/btag.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  eff = np.divide(num[0], denom[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import selection\n",
    "import weights\n",
    "import btag\n",
    "import root_pandas\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "theoretical-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Run2011A_MultiJet', 'Run2011B_MultiJet']\n",
    "mc = ['T_TuneZ2_s', 'WJetsToLNu', 'DYJetsToLL', 'T_TuneZ2_tW', 'T_TuneZ2_t-channel',\n",
    "       'Tbar_TuneZ2_s', 'Tbar_TuneZ2_tW', 'Tbar_TuneZ2_t-channel', 'TTJets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "f = ROOT.TFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pressing-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.lookup_tools import extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "elect-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\n",
    "    \"* * data/Legacy11_V1_DATA_UncertaintySources_AK5PFchs.junc.txt\",\n",
    "    \"* * jer_sf.root\",\n",
    "    \"* * JESUncMC.root\"\n",
    "])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "tropical-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\n",
    "    \"jet40_ * rootFilesTurnOn/TriggerEffHisto_data_match40_JETLEG.root\",\n",
    "    \"jet45_ * rootFilesTurnOn/TriggerEffHisto_data_match45_JETLEG.root\",\n",
    "    \"tau40_ * rootFilesTurnOn/TriggerEffHisto_match40_newTauID.root\",\n",
    "    \"tau45_ * rootFilesTurnOn/TriggerEffHisto_match45_newTauID.root\"\n",
    "])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "unnecessary-federal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['jet40_jet1_ref', 'jet40_jet1_ref_error', 'jet40_jet2_ref', 'jet40_jet2_ref_error', 'jet40_jet3_ref', 'jet40_jet3_ref_error', 'jet40_jet4_ref', 'jet40_jet4_ref_error', 'jet40_jet1_trig', 'jet40_jet1_trig_error', 'jet40_jet2_trig', 'jet40_jet2_trig_error', 'jet40_jet3_trig', 'jet40_jet3_trig_error', 'jet40_jet4_trig', 'jet40_jet4_trig_error', 'jet40_jet1_eff', 'jet40_jet1_eff_error', 'jet40_jet2_eff', 'jet40_jet2_eff_error', 'jet40_jet3_eff', 'jet40_jet3_eff_error', 'jet40_jet4_eff', 'jet40_jet4_eff_error', 'jet40_reference_tau', 'jet40_reference_tau_error', 'jet40_triggered_tau', 'jet40_triggered_tau_error', 'jet40_eff_tau', 'jet40_eff_tau_error', 'jet40_reference_tau_eta', 'jet40_reference_tau_eta_error', 'jet40_triggered_tau_eta', 'jet40_triggered_tau_eta_error', 'jet40_eff_tau_eta', 'jet40_eff_tau_eta_error', 'jet40_HLTtau_pt', 'jet40_HLTtau_pt_error', 'jet40_HLTtau_eta', 'jet40_HLTtau_eta_error', 'jet40_HLTtau_pt_diff', 'jet40_HLTtau_pt_diff_error', 'jet40_HLTtau_eta_diff', 'jet40_HLTtau_eta_diff_error', 'jet40_HLTtau_DR_diff', 'jet40_HLTtau_DR_diff_error', 'jet45_jet1_ref', 'jet45_jet1_ref_error', 'jet45_jet2_ref', 'jet45_jet2_ref_error', 'jet45_jet3_ref', 'jet45_jet3_ref_error', 'jet45_jet4_ref', 'jet45_jet4_ref_error', 'jet45_jet1_trig', 'jet45_jet1_trig_error', 'jet45_jet2_trig', 'jet45_jet2_trig_error', 'jet45_jet3_trig', 'jet45_jet3_trig_error', 'jet45_jet4_trig', 'jet45_jet4_trig_error', 'jet45_jet1_eff', 'jet45_jet1_eff_error', 'jet45_jet2_eff', 'jet45_jet2_eff_error', 'jet45_jet3_eff', 'jet45_jet3_eff_error', 'jet45_jet4_eff', 'jet45_jet4_eff_error', 'jet45_reference_tau', 'jet45_reference_tau_error', 'jet45_triggered_tau', 'jet45_triggered_tau_error', 'jet45_eff_tau', 'jet45_eff_tau_error', 'jet45_reference_tau_eta', 'jet45_reference_tau_eta_error', 'jet45_triggered_tau_eta', 'jet45_triggered_tau_eta_error', 'jet45_eff_tau_eta', 'jet45_eff_tau_eta_error', 'jet45_HLTtau_pt', 'jet45_HLTtau_pt_error', 'jet45_HLTtau_eta', 'jet45_HLTtau_eta_error', 'jet45_HLTtau_pt_diff', 'jet45_HLTtau_pt_diff_error', 'jet45_HLTtau_eta_diff', 'jet45_HLTtau_eta_diff_error', 'jet45_HLTtau_DR_diff', 'jet45_HLTtau_DR_diff_error', 'tau40_jet1_ref', 'tau40_jet1_ref_error', 'tau40_jet2_ref', 'tau40_jet2_ref_error', 'tau40_jet3_ref', 'tau40_jet3_ref_error', 'tau40_jet4_ref', 'tau40_jet4_ref_error', 'tau40_jet1_trig', 'tau40_jet1_trig_error', 'tau40_jet2_trig', 'tau40_jet2_trig_error', 'tau40_jet3_trig', 'tau40_jet3_trig_error', 'tau40_jet4_trig', 'tau40_jet4_trig_error', 'tau40_jet1_eff', 'tau40_jet1_eff_error', 'tau40_jet2_eff', 'tau40_jet2_eff_error', 'tau40_jet3_eff', 'tau40_jet3_eff_error', 'tau40_jet4_eff', 'tau40_jet4_eff_error', 'tau40_reference_tau', 'tau40_reference_tau_error', 'tau40_triggered_tau', 'tau40_triggered_tau_error', 'tau40_eff_tau', 'tau40_eff_tau_error', 'tau40_reference_tau_eta', 'tau40_reference_tau_eta_error', 'tau40_triggered_tau_eta', 'tau40_triggered_tau_eta_error', 'tau40_eff_tau_eta', 'tau40_eff_tau_eta_error', 'tau40_HLTtau_pt', 'tau40_HLTtau_pt_error', 'tau40_HLTtau_eta', 'tau40_HLTtau_eta_error', 'tau40_HLTtau_pt_diff', 'tau40_HLTtau_pt_diff_error', 'tau40_HLTtau_eta_diff', 'tau40_HLTtau_eta_diff_error', 'tau40_HLTtau_DR_diff', 'tau40_HLTtau_DR_diff_error', 'tau40_HLTTau_HLTJet_DR', 'tau40_HLTTau_HLTJet_DR_error', 'tau40_reference_tau_2', 'tau40_reference_tau_2_error', 'tau40_triggered_tau_2', 'tau40_triggered_tau_2_error', 'tau40_eff_tau_2', 'tau40_eff_tau_2_error', 'tau45_jet1_ref', 'tau45_jet1_ref_error', 'tau45_jet2_ref', 'tau45_jet2_ref_error', 'tau45_jet3_ref', 'tau45_jet3_ref_error', 'tau45_jet4_ref', 'tau45_jet4_ref_error', 'tau45_jet1_trig', 'tau45_jet1_trig_error', 'tau45_jet2_trig', 'tau45_jet2_trig_error', 'tau45_jet3_trig', 'tau45_jet3_trig_error', 'tau45_jet4_trig', 'tau45_jet4_trig_error', 'tau45_jet1_eff', 'tau45_jet1_eff_error', 'tau45_jet2_eff', 'tau45_jet2_eff_error', 'tau45_jet3_eff', 'tau45_jet3_eff_error', 'tau45_jet4_eff', 'tau45_jet4_eff_error', 'tau45_reference_tau', 'tau45_reference_tau_error', 'tau45_triggered_tau', 'tau45_triggered_tau_error', 'tau45_eff_tau', 'tau45_eff_tau_error', 'tau45_reference_tau_eta', 'tau45_reference_tau_eta_error', 'tau45_triggered_tau_eta', 'tau45_triggered_tau_eta_error', 'tau45_eff_tau_eta', 'tau45_eff_tau_eta_error', 'tau45_HLTtau_pt', 'tau45_HLTtau_pt_error', 'tau45_HLTtau_eta', 'tau45_HLTtau_eta_error', 'tau45_HLTtau_pt_diff', 'tau45_HLTtau_pt_diff_error', 'tau45_HLTtau_eta_diff', 'tau45_HLTtau_eta_diff_error', 'tau45_HLTtau_DR_diff', 'tau45_HLTtau_DR_diff_error', 'tau45_HLTTau_HLTJet_DR', 'tau45_HLTTau_HLTJet_DR_error', 'tau45_reference_tau_2', 'tau45_reference_tau_2_error', 'tau45_triggered_tau_2', 'tau45_triggered_tau_2_error', 'tau45_eff_tau_2', 'tau45_eff_tau_2_error'])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dried-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_weight(obj, key):\n",
    "    \n",
    "    weight = evaluator[key](obj)\n",
    "    mask = weight == 0.\n",
    "    return np.where(mask, 1., weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ceramic-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(jet, tau, prefix):\n",
    "    \n",
    "    jet0 = eval_weight(jet.pt[:,0], \"jet\" + prefix + \"_jet4_eff\" )\n",
    "    jet1 = eval_weight(jet.pt[:,1], \"jet\" + prefix + \"_jet4_eff\" )\n",
    "    jet2 = eval_weight(jet.pt[:,2], \"jet\" + prefix + \"_jet4_eff\" )\n",
    "    tau0 = eval_weight(tau.pt[:,0], \"tau\" + prefix + \"_eff_tau\" )\n",
    "    \n",
    "    jet0_err = eval_weight(jet.pt[:,0], \"jet\" + prefix + \"_jet4_eff_error\" )\n",
    "    jet1_err = eval_weight(jet.pt[:,1], \"jet\" + prefix + \"_jet4_eff_error\" )\n",
    "    jet2_err = eval_weight(jet.pt[:,2], \"jet\" + prefix + \"_jet4_eff_error\" )\n",
    "    tau0_err = eval_weight(tau.pt[:,0], \"tau\" + prefix + \"_eff_tau_error\" )\n",
    "    \n",
    "    weight = 1. * jet0 * jet1 * jet2 * tau0\n",
    "    weight_up = 1. * (jet0 + jet0_err) * (jet1 + jet1_err) * (jet2 + jet2_err) * (tau0 + tau0_err)\n",
    "    weight_down = 1. * (jet0 - jet0_err) * (jet1 - jet1_err) * (jet2 - jet2_err) * (tau0 - tau0_err)\n",
    "    \n",
    "    return weight, weight_up, weight_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "jewish-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def trigger_weight(jet, tau, frac=0.218):\n",
    "    \n",
    "    # Randomly select 40/45 trigger according to lumi fraction\n",
    "    rand = np.random.random((len(jet)))\n",
    "    mask_is40 = rand < frac\n",
    "    \n",
    "    weight_40, weight_up_40, weight_down_40 = calc_weight(jet, tau, \"40\")\n",
    "    weight_45, weight_up_45, weight_down_45 = calc_weight(jet, tau, \"45\")\n",
    "    \n",
    "    weight = np.where(mask_is40, weight_40, weight_45)\n",
    "    weight_up = np.where(mask_is40, weight_up_40, weight_up_45)\n",
    "    weight_down = np.where(mask_is40, weight_down_45, weight_down_45)\n",
    "    \n",
    "    return weight, weight_up, weight_down\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "lonely-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = trigger_weight(jet, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "communist-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ROOT.TFile(\"data/beff.root\")\n",
    "h2_BTaggingEff_Denom_b = f.Get(\"h2_BTaggingEff_Denom_b\")\n",
    "h2_BTaggingEff_Num_b = f.Get(\"h2_BTaggingEff_Num_b\")\n",
    "h2_BTaggingEff_Denom_c = f.Get(\"h2_BTaggingEff_Denom_c\")\n",
    "h2_BTaggingEff_Num_c = f.Get(\"h2_BTaggingEff_Num_c\")\n",
    "h2_BTaggingEff_Denom_usdg = f.Get(\"h2_BTaggingEff_Denom_udsg\")\n",
    "h2_BTaggingEff_Num_usdg = f.Get(\"h2_BTaggingEff_Num_udsg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "manual-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_eff(h1, h2):\n",
    "    \n",
    "    h3 = h1.Clone(\"h3\")\n",
    "    h3.Sumw2()\n",
    "    h3.Divide(h2)\n",
    "    return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "disturbed-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = ROOT.TFile(\"data/beff_precalc.root\", \"RECREATE\")\n",
    "b = to_eff(h2_BTaggingEff_Num_b, h2_BTaggingEff_Denom_b)\n",
    "c = to_eff(h2_BTaggingEff_Num_c, h2_BTaggingEff_Denom_c)\n",
    "usdg = to_eff(h2_BTaggingEff_Num_usdg, h2_BTaggingEff_Denom_usdg)\n",
    "b.Write(\"b\")\n",
    "c.Write(\"c\")\n",
    "usdg.Write(\"usdg\")\n",
    "outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "devoted-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = ROOT.TFile(\"data/beff_precalc.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "blessed-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\n",
    "    \"* * data/beff_precalc.root\"\n",
    "])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "loving-investor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['b', 'b_error', 'c', 'c_error', 'usdg', 'usdg_error'])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "statistical-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "import root_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-transcript",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "sustained-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = ff.Get(\"b\")\n",
    "h2 = ff.Get(\"c\")\n",
    "h3 = ff.Get(\"usdg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "strong-rogers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   0.,   10.,   20.,   30.,   40.,   50.,   60.,   70.,   80.,\n",
       "          90.,  100.,  110.,  120.,  130.,  140.,  150.,  160.,  170.,\n",
       "         180.,  190.,  200.,  210.,  220.,  230.,  240.,  250.,  260.,\n",
       "         270.,  280.,  290.,  300.,  310.,  320.,  330.,  340.,  350.,\n",
       "         360.,  370.,  380.,  390.,  400.,  410.,  420.,  430.,  440.,\n",
       "         450.,  460.,  470.,  480.,  490.,  500.,  510.,  520.,  530.,\n",
       "         540.,  550.,  560.,  570.,  580.,  590.,  600.,  610.,  620.,\n",
       "         630.,  640.,  650.,  660.,  670.,  680.,  690.,  700.,  710.,\n",
       "         720.,  730.,  740.,  750.,  760.,  770.,  780.,  790.,  800.,\n",
       "         810.,  820.,  830.,  840.,  850.,  860.,  870.,  880.,  890.,\n",
       "         900.,  910.,  920.,  930.,  940.,  950.,  960.,  970.,  980.,\n",
       "         990., 1000.]),\n",
       " array([-3. , -2.9, -2.8, -2.7, -2.6, -2.5, -2.4, -2.3, -2.2, -2.1, -2. ,\n",
       "        -1.9, -1.8, -1.7, -1.6, -1.5, -1.4, -1.3, -1.2, -1.1, -1. , -0.9,\n",
       "        -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,  0.2,\n",
       "         0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,\n",
       "         1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,\n",
       "         2.5,  2.6,  2.7,  2.8,  2.9,  3. ])]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "baking-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_num, edges = root_numpy.hist2array(h1, return_edges=True)\n",
    "c_num, _ = root_numpy.hist2array(h2, return_edges=True)\n",
    "usdg_num, _ = root_numpy.hist2array(h3, return_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "international-amazon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 60)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "amino-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = np.dstack((b_num, c_num, usdg_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "statutory-oliver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 60, 3)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "sitting-consistency",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-0c964dfa8759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "sf[jet.pt, jet.eta, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "dental-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TFile::Append>: Replacing existing TH1: hist (Potential memory leak).\n"
     ]
    }
   ],
   "source": [
    "hist = ROOT.TH3F(\"hist\", \"hist\", 100, 0., 1000., 60, -3., 3., 3, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "compressed-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from root_numpy import array2hist, hist2array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "parental-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = array2hist(sf, hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "committed-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "outtest = ROOT.TFile(\"data/beff_test.root\", \"RECREATE\")\n",
    "hist.Write(\"eff\")\n",
    "outtest.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "comparative-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = ROOT.TFile(\"data/beff_test.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "residential-capture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFile**\t\tdata/beff_test.root\t\n",
      " TFile*\t\tdata/beff_test.root\t\n",
      "  KEY: TH3F\teff;1\thist\n"
     ]
    }
   ],
   "source": [
    "fff.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "becoming-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "ext.add_weight_sets([\n",
    "    \"* * data/beff_test.root\"\n",
    "])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "broad-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['eff', 'eff_error'])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "undefined-commitment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3 dimensional histogram with axes:\n",
       "\t1: [   0.   10.   20.   30.   40.   50.   60.   70.   80.   90.  100.  110.\n",
       "  120.  130.  140.  150.  160.  170.  180.  190.  200.  210.  220.  230.\n",
       "  240.  250.  260.  270.  280.  290.  300.  310.  320.  330.  340.  350.\n",
       "  360.  370.  380.  390.  400.  410.  420.  430.  440.  450.  460.  470.\n",
       "  480.  490.  500.  510.  520.  530.  540.  550.  560.  570.  580.  590.\n",
       "  600.  610.  620.  630.  640.  650.  660.  670.  680.  690.  700.  710.\n",
       "  720.  730.  740.  750.  760.  770.  780.  790.  800.  810.  820.  830.\n",
       "  840.  850.  860.  870.  880.  890.  900.  910.  920.  930.  940.  950.\n",
       "  960.  970.  980.  990. 1000.]\n",
       "\t2: [-3.  -2.9 -2.8 -2.7 -2.6 -2.5 -2.4 -2.3 -2.2 -2.1 -2.  -1.9 -1.8 -1.7\n",
       " -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.  -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3\n",
       " -0.2 -0.1  0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.   1.1\n",
       "  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.   2.1  2.2  2.3  2.4  2.5\n",
       "  2.6  2.7  2.8  2.9  3. ]\n",
       "\t3: [0. 1. 2. 3.]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator[\"eff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "amateur-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012711762"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator[\"eff\"](100., 0.15, 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "afraid-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "subsequent-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau p4 TLorentzVector(51.164, -0.59398, 1.7806, 0.87758)\n",
      "Added TLorentzVector(-10.796, -59.256, -115.75, 571.68)\n",
      "Tau p4 TLorentzVector(45.569, 2.2877, -2.4951, 0.81105)\n",
      "Added TLorentzVector(9.0903, 46.581, 845.48, 1143.7)\n",
      "Tau p4 TLorentzVector(67.135, 0.97985, 0.10675, 1.2852)\n",
      "Added TLorentzVector(-67.808, 78.609, 294.41, 1345.4)\n",
      "Tau p4 TLorentzVector(48.314, 0.89961, 1.1036, 0.13957)\n",
      "Added TLorentzVector(-0.61799, 14.496, 82.415, 435.71)\n",
      "Tau p4 TLorentzVector(76.19, -0.64243, -2.304, 1.082)\n",
      "Added TLorentzVector(-8.1471, 4.62, -1231.4, 1386.4)\n",
      "Tau p4 TLorentzVector(56.506, 0.52762, -2.8298, 0.53326)\n",
      "Added TLorentzVector(3.5307, 9.2098, 145.16, 389.05)\n",
      "Tau p4 TLorentzVector(74.513, 0.22192, -2.2742, 0.62249)\n",
      "Added TLorentzVector(36.168, -6.963, 115.37, 456.52)\n",
      "Tau p4 TLorentzVector(131.81, -0.34514, 0.21226, 0.13957)\n",
      "Added TLorentzVector(-108.85, -9.6578, -70.147, 797.66)\n",
      "Tau p4 TLorentzVector(76.648, 0.15379, 0.17856, 0.99302)\n",
      "Added TLorentzVector(-306.62, -87.184, -582.58, 1310.6)\n",
      "Tau p4 TLorentzVector(46.719, -0.37272, 1.5811, 0.69335)\n",
      "Added TLorentzVector(-31.406, -123.13, -201.01, 452.65)\n",
      "Tau p4 TLorentzVector(61.08, -0.29866, 2.7581, 0.64257)\n",
      "Added TLorentzVector(30.322, -7.8337, 147.04, 468.88)\n",
      "Tau p4 TLorentzVector(87.232, -1.9326, -2.9247, 1.1736)\n",
      "Added TLorentzVector(29.043, -109.7, 170.88, 949.72)\n",
      "Tau p4 TLorentzVector(51.175, -0.97449, 0.61511, 1.1109)\n",
      "Added TLorentzVector(-28.674, 30.862, -1280.5, 1561.3)\n",
      "Tau p4 TLorentzVector(84.497, -1.4062, -1.2449, 0.62197)\n",
      "Added TLorentzVector(-23.048, 64.857, -395.71, 599.22)\n",
      "Tau p4 TLorentzVector(46.501, 0.092026, 2.9389, 0.84451)\n",
      "Added TLorentzVector(23.707, -32.542, 11.401, 414.16)\n",
      "Tau p4 TLorentzVector(47.54, -1.4069, -2.0992, 0.7816)\n",
      "Added TLorentzVector(10.974, 5.0479, -89.484, 650.14)\n",
      "Tau p4 TLorentzVector(49.128, 1.9161, -2.9255, 1.3991)\n",
      "Added TLorentzVector(-52.261, 42.044, 737.6, 847.37)\n",
      "Tau p4 TLorentzVector(52.477, -1.9671, -0.44922, 0.67501)\n",
      "Added TLorentzVector(55.769, 80.831, -570.87, 743.37)\n",
      "Tau p4 TLorentzVector(71.872, -0.79244, -2.1402, 0.71579)\n",
      "Added TLorentzVector(67.105, -16.757, -506.91, 713.06)\n",
      "Tau p4 TLorentzVector(86.265, 0.14034, -1.3902, 0.63731)\n",
      "Added TLorentzVector(-18.498, -29.168, -149.44, 742.1)\n",
      "Tau p4 TLorentzVector(77.624, -0.17133, -2.8646, 0.97213)\n",
      "Added TLorentzVector(-35.874, -42.603, 274.66, 637.18)\n",
      "Tau p4 TLorentzVector(47.035, 0.26696, 0.52658, 0.13957)\n",
      "Added TLorentzVector(19.008, -16.911, -343.06, 638.86)\n",
      "Tau p4 TLorentzVector(67.269, 0.96695, -2.4342, 0.83079)\n",
      "Added TLorentzVector(10.779, 29.919, 277.41, 487.81)\n",
      "Tau p4 TLorentzVector(87.763, 0.7573, -0.76907, 0.13957)\n",
      "Added TLorentzVector(-37.615, 27.272, 271.39, 595.52)\n",
      "Tau p4 TLorentzVector(51.416, 0.47135, -2.2667, 0.13957)\n",
      "Added TLorentzVector(-9.6387, -4.7054, 172.45, 485.52)\n",
      "Tau p4 TLorentzVector(91.651, -0.63236, -2.2272, 0.57114)\n",
      "Added TLorentzVector(-51.867, -53.21, -641.06, 874.18)\n",
      "Tau p4 TLorentzVector(94.646, 1.4716, -0.59911, 0.45123)\n",
      "Added TLorentzVector(-117.39, 53.481, 760.85, 1178.8)\n",
      "Tau p4 TLorentzVector(52.637, 1.1859, -3.0942, 1.2053)\n",
      "Added TLorentzVector(158.07, 117.79, 481.04, 779.54)\n",
      "Tau p4 TLorentzVector(57.585, 0.29644, -0.16377, 1.163)\n",
      "Added TLorentzVector(51.189, 20.914, 284.39, 552.36)\n",
      "Tau p4 TLorentzVector(64, -0.58618, -1.0014, 1.4476)\n",
      "Added TLorentzVector(-0.46475, 38.362, 30.793, 540.73)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for j, t in zip(jet, tau):\n",
    "    \n",
    "    p4 = copy.deepcopy(t[\"p4\"][0])\n",
    "    print(\"Tau p4\", p4)\n",
    "    HT, H = 0., 0.\n",
    "    for vec in j:\n",
    "        p4 += vec[\"p4\"]\n",
    "    print(\"Added\", p4)\n",
    "    #print( j[\"p4\"].pt, t[\"p4\"].pt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-crystal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-tuning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cloudy-anatomy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PtEtaPhiMassLorentzVectorArray [TLorentzVector(163.2, -0.019414, 0.40543, 16.03) TLorentzVector(147.07, -0.5662, -2.1255, 17.739) TLorentzVector(87.174, 0.60444, -3.1305, 15.547) TLorentzVector(49.84, -0.86283, -1.2753, 9.7447)] at 0x7fc9d4bb41d0>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet[\"p4\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "actual-contemporary",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot add a TLorentzVector with a int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-3fb6c596e8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"p4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_97apython3/x86_64-centos7-gcc8-opt/lib/python3.7/site-packages/uproot_methods/classes/TLorentzVector.py\u001b[0m in \u001b[0;36m__radd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__radd__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_97apython3/x86_64-centos7-gcc8-opt/lib/python3.7/site-packages/uproot_methods/classes/TLorentzVector.py\u001b[0m in \u001b[0;36m_vector\u001b[0;34m(self, operator, vector, reverse)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mcart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_cartesian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMethods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot {0} a TLorentzVector with a {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTLorentzVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot add a TLorentzVector with a int"
     ]
    }
   ],
   "source": [
    "sum(jet[\"p4\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-wildlife",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-numbers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-consideration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "essential-drain",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "starts and stops are not compatible with a single offsets array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-13901e1a1738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usdg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/coffea/lookup_tools/lookup_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All input jagged arrays must have a common structure (offsets)!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_97apython3/x86_64-centos7-gcc8-opt/lib/python3.7/site-packages/awkward/array/jagged.py\u001b[0m in \u001b[0;36moffsets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_starts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"starts and stops are not compatible with a single offsets array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: starts and stops are not compatible with a single offsets array"
     ]
    }
   ],
   "source": [
    "evaluator[\"usdg\"](jet.pt, jet.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-australia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-soviet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "subject-friend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['jet40', 'jet45', 'tau40', 'tau45'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "active-tolerance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672131"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator[\"jet40\"](100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "funny-python",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: T_TuneZ2_s.root isData: False isTT: False corrLevel centJER\n",
      "SF 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(selection)\n",
    "path = \"/eos/user/l/llayer/opendata_files/preselection_merged/\" + \"T_TuneZ2_s\" + \".root\"\n",
    "df, jet, tau, cut_flow = selection.event_selection(path, isData = False, isTT = False, corrLevel = \"centJER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "figured-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([164.9741  , 227.10472 , 298.7569  ,  99.604546, 139.6582  ,\n",
       "        79.38644 ,  89.59161 , 343.78494 , 447.89755 , 118.52118 ,\n",
       "       119.038055, 169.07281 , 189.04832 ,  98.94595 , 109.097916,\n",
       "       194.0468  , 172.73466 , 150.04248 , 143.88855 , 172.40742 ,\n",
       "       100.71692 , 117.78187 ,  97.741745, 178.94006 ,  63.02036 ,\n",
       "       110.818504, 212.15852 , 242.76344 , 132.96727 , 106.133865],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet.pt[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "sharp-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet40 = evaluator[\"jet40\"](jet.pt[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "functional-mechanism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.        , 0.94067794, 0.9166667 ,\n",
       "       0.90782124, 0.93877554, 0.        , 0.        , 0.93333334,\n",
       "       0.93333334, 1.        , 1.        , 0.94067794, 0.9672131 ,\n",
       "       0.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.9672131 , 0.93333334, 0.94067794, 1.        , 0.89365673,\n",
       "       0.93333334, 0.        , 0.        , 0.9166667 , 0.9672131 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "unique-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = jet40 == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "global-hazard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, False, False, False, False,  True,  True,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True, False, False])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "patent-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet40 = np.where(mask, 1., jet40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "boring-shark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 0.94067794, 0.9166667 ,\n",
       "       0.90782124, 0.93877554, 1.        , 1.        , 0.93333334,\n",
       "       0.93333334, 1.        , 1.        , 0.94067794, 0.9672131 ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.9672131 , 0.93333334, 0.94067794, 1.        , 0.89365673,\n",
       "       0.93333334, 1.        , 1.        , 0.9166667 , 0.9672131 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_weight(jet, tau):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "altered-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.random((len(jet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "pacific-cheese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "        True, False, False])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t < 0.218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "psychological-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.where(t < 0.218, 0.7, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "graphic-defeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.7, 0.3, 0.3, 0.3, 0.3,\n",
       "       0.3, 0.3, 0.3, 0.3, 0.3, 0.7, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.7,\n",
       "       0.3, 0.7, 0.3, 0.3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = np.where(t < 0.218, 0.7, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-suggestion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-asthma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-insider",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-angle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "broken-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: T_TuneZ2_s.root isData: False isTT: False corrLevel cent\n",
      "{'preselected': 1915, 'hlt': 273, 'lep_veto': 190, 'jet_requirement': 114, 'tau_requirement': 30}\n",
      "Processing: WJetsToLNu.root isData: False isTT: False corrLevel cent\n",
      "{'preselected': 44906, 'hlt': 4982, 'lep_veto': 2385, 'jet_requirement': 1083, 'tau_requirement': 231}\n",
      "Processing: DYJetsToLL.root isData: False isTT: False corrLevel cent\n",
      "{'preselected': 114807, 'hlt': 19961, 'lep_veto': 5461, 'jet_requirement': 2265, 'tau_requirement': 547}\n",
      "Processing: T_TuneZ2_tW.root isData: False isTT: False corrLevel cent\n",
      "{'preselected': 20110, 'hlt': 5529, 'lep_veto': 3046, 'jet_requirement': 1698, 'tau_requirement': 568}\n",
      "Processing: T_TuneZ2_t-channel.root isData: False isTT: False corrLevel cent\n",
      "{'preselected': 23527, 'hlt': 3396, 'lep_veto': 2404, 'jet_requirement': 1203, 'tau_requirement': 240}\n",
      "Processing: Tbar_TuneZ2_s.root isData: False isTT: False corrLevel cent\n",
      "{'preselected': 1117, 'hlt': 158, 'lep_veto': 115, 'jet_requirement': 61, 'tau_requirement': 12}\n",
      "Processing: Tbar_TuneZ2_tW.root isData: False isTT: False corrLevel cent\n",
      "{'preselected': 21966, 'hlt': 6034, 'lep_veto': 3317, 'jet_requirement': 1902, 'tau_requirement': 648}\n",
      "Processing: Tbar_TuneZ2_t-channel.root isData: False isTT: False corrLevel cent\n",
      "{'preselected': 15056, 'hlt': 2145, 'lep_veto': 1534, 'jet_requirement': 786, 'tau_requirement': 144}\n",
      "Processing: TTJets.root isData: False isTT: True corrLevel cent\n",
      "{'preselected': 1769852, 'hlt': 654451, 'lep_veto': 347759, 'jet_requirement': 228585, 'tau_requirement': 87196}\n",
      "Processing: Run2011A_MultiJet.root isData: True isTT: False corrLevel cent\n",
      "{'preselected': 340427, 'trigger': 301139, 'hlt': 301139, 'lep_veto': 286498, 'jet_requirement': 132253, 'tau_requirement': 13741}\n",
      "Processing: Run2011B_MultiJet.root isData: True isTT: False corrLevel cent\n",
      "{'preselected': 322288, 'trigger': 218730, 'hlt': 218730, 'lep_veto': 208262, 'jet_requirement': 91877, 'tau_requirement': 9892}\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(selection)\n",
    "def event_selection():\n",
    "    \n",
    "    samples = {}\n",
    "    for sample in mc + data:\n",
    "        \n",
    "        #!!!!!!! Careful with JER application before Tau?!\n",
    "        \n",
    "        if \"TTJets\" in sample: isTT = True\n",
    "        else: isTT = False\n",
    "            \n",
    "        if \"Run2011\" in sample: isData = True\n",
    "        else: isData = False\n",
    "        \n",
    "        path = \"/eos/user/l/llayer/opendata_files/preselection_merged/\" + sample + \".root\"\n",
    "        \n",
    "        df, cut_flow = selection.event_selection(path, isData = isData, isTT = isTT)\n",
    "        \n",
    "        print( cut_flow )\n",
    "        \"\"\"\n",
    "        df.to_hdf(outpath + sample + \".h5\", \"central\", mode='w')\n",
    "                \n",
    "        if isData == False:\n",
    "            \n",
    "            for c in corrections:\n",
    "                df, cut_flow = selection.event_selection(path, isData = isData, isTT = isTT, corrLevel = c)\n",
    "                print( cut_flow )\n",
    "                df.to_hdf(outpath + sample + \".h5\", c, mode='a')  \n",
    "        \"\"\"\n",
    "        \n",
    "        samples[sample] = df\n",
    "        \n",
    "    return samples\n",
    "\n",
    "samples = event_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "imposed-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/eos/home-l/llayer/cmsopen/columnar/btag.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  eff = np.divide(num[0], denom[0])\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(btag)\n",
    "importlib.reload(weights)\n",
    "\n",
    "def candidates(sample, df, invert_btag = False, njets=-1):\n",
    "    \n",
    "    if \"Run2011\" in sample: isData = True\n",
    "    else: isData = False\n",
    "\n",
    "    print( \"Processing:\", sample,\"isData:\", isData, \"invert_btag:\", invert_btag)\n",
    "\n",
    "    df['nJets'] = df[\"Jet_pt\"].str.len()\n",
    "    \n",
    "    # b-tagging\n",
    "    df[\"Jet_nbtags\"] = df[\"Jet_csvDisc\"].apply( lambda x : btag.count_btags(x, njets=njets) )\n",
    "    if invert_btag:\n",
    "        df = btag.no_tag(df)\n",
    "    else:\n",
    "        df = btag.at_least_1tag(df)\n",
    "\n",
    "    # MET cut\n",
    "    df = selection.met_requirement(df)\n",
    "\n",
    "    \n",
    "    # HL features\n",
    "    #df = pd.concat([df, df.apply(lambda ev : pd.Series(hl.hlFeatures(ev, njets=njets)), axis=1)], axis=1)\n",
    "    \n",
    "\n",
    "    # MC weights\n",
    "    if not isData:\n",
    "\n",
    "        hlt_40, hlt_45 = weights.lumi()\n",
    "        total_lumi = hlt_40 + hlt_45\n",
    "        trigger_frac = hlt_40 / float(hlt_45)\n",
    "        df = pd.concat([df, df.apply(lambda ev: pd.Series(btag.eval_sf_eff(ev)), axis=1)], axis=1)\n",
    "        df[\"Jet_btag_weight1\"] = df.apply(lambda ev : btag.b_weight_method1(ev, njets=njets), axis=1)\n",
    "        #df[\"Jet_btag_weight2\"] = df.apply(lambda ev : btag.b_weight_method2(ev, njets=njets), axis=1)\n",
    "        # trigger weights\n",
    "        #df[\"trigger_weight\"] = df.apply(lambda ev : weights.trigger_weight(ev, trigger_frac), axis=1)\n",
    "        df = pd.concat([df, df.apply(lambda ev: pd.Series(weights.trigger_weight(ev, trigger_frac)), axis=1)], axis=1)\n",
    "        # normalization\n",
    "        counts_path = \"/eos/user/l/llayer/opendata_files/preselection_merged/\" + sample + \"_counts.root\"\n",
    "        total_counts = root_pandas.read_root(counts_path)\n",
    "        xsec = weights.get_xsec(sample)\n",
    "        weights.norm(df, total_counts, xsec, lumi = total_lumi)\n",
    "\n",
    "    # QCD\n",
    "    if isData & invert_btag:\n",
    "        \n",
    "        # Assume light flavour\n",
    "        def lf(nJets):\n",
    "            return np.zeros((nJets))\n",
    "        df[\"Jet_flavour\"] = df[\"nJets\"].apply(lf)\n",
    "        df = pd.concat([df, df.apply(lambda ev: pd.Series(btag.eval_sf_eff(ev)), axis=1)], axis=1)\n",
    "        df[\"btag_weight\"] = df.apply(lambda ev : btag.b_weight_method2(ev, njets=njets), axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "received-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_samples(samples):\n",
    "    \n",
    "    # Concat and split MC in signal and background\n",
    "    new_samples = {}\n",
    "    new_samples[\"TTJets_signal\"], new_samples[\"TTJets_bkg\"] = weights.classify_tt(samples[\"TTJets\"])\n",
    "    new_samples[\"WZJets\"] = pd.concat([samples['WJetsToLNu'], samples['DYJetsToLL']], axis=0)\n",
    "    new_samples[\"STJets\"] = pd.concat([samples['T_TuneZ2_s'], samples['T_TuneZ2_tW'], samples['T_TuneZ2_t-channel'], \n",
    "                              samples['Tbar_TuneZ2_s'], samples['Tbar_TuneZ2_tW'], \n",
    "                              samples['Tbar_TuneZ2_t-channel']], axis=0)\n",
    "\n",
    "    # Concat the data\n",
    "    new_samples[\"Data\"] = pd.concat([samples[\"Run2011A_MultiJet\"], samples[\"Run2011B_MultiJet\"]], axis=0)\n",
    "    new_samples[\"QCD\"] = pd.concat([samples[\"QCD_Run2011A_MultiJet\"], samples[\"QCD_Run2011B_MultiJet\"]], axis=0)\n",
    "    \n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "processed-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_candidates(samples, njets = -1):\n",
    "    \n",
    "    cand_samples = {}\n",
    "    for sample in data:\n",
    "        cand_samples[sample] = candidates(sample, samples[sample], invert_btag = False, njets=njets)\n",
    "        cand_samples[\"QCD_\" + sample] = candidates(sample, samples[sample], invert_btag = True, njets=njets)\n",
    "    \n",
    "    \n",
    "    for sample in mc:\n",
    "        cand_samples[sample] = candidates(sample, samples[sample], invert_btag = False, njets=njets)   \n",
    "\n",
    "    new_samples = rearrange_samples(cand_samples)\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "south-powder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Run2011A_MultiJet isData: True invert_btag: False\n",
      "Processing: Run2011A_MultiJet isData: True invert_btag: True\n",
      "Processing: Run2011B_MultiJet isData: True invert_btag: False\n",
      "Processing: Run2011B_MultiJet isData: True invert_btag: True\n",
      "Processing: T_TuneZ2_s isData: False invert_btag: False\n",
      "Processing: WJetsToLNu isData: False invert_btag: False\n",
      "Processing: DYJetsToLL isData: False invert_btag: False\n",
      "Processing: T_TuneZ2_tW isData: False invert_btag: False\n",
      "Processing: T_TuneZ2_t-channel isData: False invert_btag: False\n",
      "Processing: Tbar_TuneZ2_s isData: False invert_btag: False\n",
      "Processing: Tbar_TuneZ2_tW isData: False invert_btag: False\n",
      "Processing: Tbar_TuneZ2_t-channel isData: False invert_btag: False\n",
      "Processing: TTJets isData: False invert_btag: False\n"
     ]
    }
   ],
   "source": [
    "cands = proc_candidates(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "controlled-payday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3238"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cands[\"Data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "outdoor-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from root_numpy import fill_hist\n",
    "\n",
    "def save_var(sample, name, var_name, bins = 20, xlow = 0., xup = 350):\n",
    "\n",
    "    hist = ROOT.TH1D(name + \"_\" + var_name, name + \"_\" + var_name, bins, xlow, xup)\n",
    "    hist.Sumw2()\n",
    "    \n",
    "    if name == \"Data\":\n",
    "        pass\n",
    "    elif name == \"QCD\":\n",
    "        if var_name == \"bdt\":\n",
    "            scale_qcd = 9.\n",
    "            sample = sample[sample[\"train_flag\"] == \"test\"]\n",
    "        else:\n",
    "            scale_qcd = 4.3\n",
    "        sample['weight'] = sample['btag_weight'] * scale_qcd\n",
    "    else:\n",
    "        #samples[sample]['new_trigger_weight'] = new_samples[sample].apply(lambda ev : weights.trigger_weight(ev), axis=1)\n",
    "        sample['weight'] = sample['norm'] * (1/1000) * sample['trigger_weight'] * sample['Jet_btag_weight1']\n",
    "        #print(sample, sum(samples[sample]['weight']))\n",
    "        #new_samples[sample]['btag_weight2']\n",
    "    \n",
    "    # Flatten if the column is a list\n",
    "    if \"Jet_\" in var_name:\n",
    "        series = sample[var_name].apply(pd.Series).stack().reset_index(drop=True)\n",
    "        if name != \"Data\":\n",
    "            sample['weight_stacked'] = sample.apply(lambda x : stack_weight(x[\"weight\"] ,x[\"nJets\"]), axis=1)\n",
    "            weights = sample['weight_stacked'].apply(pd.Series).stack().reset_index(drop=True)\n",
    "    else:\n",
    "        series = sample[var_name]\n",
    "        if name != \"Data\":\n",
    "            weights = sample[\"weight\"]\n",
    "    if name == \"Data\":\n",
    "        fill_hist(hist, series)\n",
    "    else:\n",
    "        fill_hist(hist, series, weights = weights)\n",
    "    #hist.Write()\n",
    "    return hist\n",
    "\n",
    "def vars_to_histos(samples, variables):\n",
    "    \n",
    "    hists = {}\n",
    "    for name, sample in samples.items():\n",
    "        for var in variables:\n",
    "            hists[name + \"_\" + var[\"var_name\"]] = save_var(sample, name, var[\"var_name\"], var[\"bins\"], var[\"xlow\"], var[\"xup\"])\n",
    "\n",
    "    return hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adaptive-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    {\"var_name\" : \"MET_met\", \"bins\" : 30, \"xlow\" : 0., \"xup\" : 400, \"xtitle\" : \"MET [GeV]\"}\n",
    "]\n",
    "\n",
    "hists = vars_to_histos(cands, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "objective-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTJets_signal_MET_met 345.9627057153264\n",
      "TTJets_bkg_MET_met 145.5093453003518\n",
      "WZJets_MET_met 61.43496068440135\n",
      "STJets_MET_met 30.534209478997937\n",
      "Data_MET_met 3238.0\n",
      "QCD_MET_met 2731.115160766783\n"
     ]
    }
   ],
   "source": [
    "f = ROOT.TFile(\"histos_test.root\", \"RECREATE\")\n",
    "for s, hist in hists.items():\n",
    "\n",
    "    print( s, hist.Integral() )\n",
    "    hist.Write()\n",
    "    \n",
    "f.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "correct-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(histos, fit_var, corr = \"central\"):\n",
    "        \n",
    "\n",
    "    bkg = histos['TTJets_bkg' + \"_\" + fit_var ].Clone()\n",
    "    bkg.Add(histos['WZJets' + \"_\" + fit_var ])\n",
    "    bkg.Add(histos['STJets' + \"_\" + fit_var ])\n",
    "\n",
    "    data = histos[\"Data\" + \"_\" + fit_var ].Clone()\n",
    "    data.Add(bkg, -1.)\n",
    "\n",
    "    signal = histos[\"TTJets_signal\" + \"_\" + fit_var ]\n",
    "\n",
    "    qcd = histos[\"QCD\" + \"_\" + fit_var ]\n",
    "\n",
    "    x = ROOT.RooRealVar(\"x\",\"x\",0.,350.)\n",
    "    rooSignal =  ROOT.RooDataHist(\"signal\",\"signal\",ROOT.RooArgList( x ), signal)\n",
    "    rooBkg = ROOT.RooDataHist(\"bkg\",\"bkg\", ROOT.RooArgList( x ), qcd)\n",
    "    signal_pdf = ROOT.RooHistPdf(\"signal\",\"signal\",ROOT.RooArgSet( x ), rooSignal)\n",
    "    bkg_pdf = ROOT.RooHistPdf(\"bkg\",\"bkg\",ROOT.RooArgSet( x ), rooBkg)\n",
    "\n",
    "    c0 = ROOT.RooRealVar(\"c0\",\"c0\",0.5,0.,1.)\n",
    "    pdf = ROOT.RooAddPdf(\"pdf\",\"pdf\", signal_pdf,bkg_pdf, c0)\n",
    "\n",
    "    dataFit = ROOT.RooDataHist(\"data\",\"data\",ROOT.RooArgList( x ), data);\n",
    "\n",
    "    fitResult = pdf.fitTo(dataFit)\n",
    "    #print( fitResult )\n",
    "\n",
    "    sf_tt_sig = (c0.getVal() * data.Integral()) / signal.Integral() \n",
    "    sf_tt_sig_err = (c0.getError() * data.Integral()) / signal.Integral()\n",
    "    \n",
    "    sf_qcd = ((1-c0.getVal())*data.Integral()) / qcd.Integral()\n",
    "    sf_qcd_err = (c0.getError() * data.Integral()) / qcd.Integral()\n",
    "    \n",
    "    \n",
    "    print( \"scale factor TTbar tau(h) QQ \", sf_tt_sig, \"+-\", sf_tt_sig_err )\n",
    "    print( \"scale factor MultiJet \", sf_qcd, \"+-\", sf_qcd_err )\n",
    "\n",
    "    return sf_tt_sig, sf_qcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "accomplished-death",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale factor TTbar tau(h) QQ  0.9910458961661615 +- 0.09467095587401728\n",
      "scale factor MultiJet  0.97667607748668 +- 0.011718521532530633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9910458961661615, 0.97667607748668)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#1] INFO:DataHandling -- RooDataHist::adjustBinning(signal): fit range of variable x expanded to nearest bin boundaries: [0,350] --> [0,360]\n",
      "[#0] WARNING:InputArguments -- RooAbsPdf::fitTo(pdf) WARNING: a likelihood fit is requested of what appears to be weighted data.\n",
      "       While the estimated values of the parameters will always be calculated taking the weights into account,\n",
      "       there are multiple ways to estimate the errors of the parameters. You are advised to make an \n",
      "       explicit choice for the error calculation:\n",
      "           - Either provide SumW2Error(true), to calculate a sum-of-weights-corrected HESSE error matrix\n",
      "             (error will be proportional to the number of events in MC).\n",
      "           - Or provide SumW2Error(false), to return errors from original HESSE error matrix\n",
      "             (which will be proportional to the sum of the weights, i.e., a dataset with <sum of weights> events).\n",
      "           - Or provide AsymptoticError(true), to use the asymptotically correct expression\n",
      "             (for details see https://arxiv.org/abs/1911.01303).\n",
      "[#1] INFO:Minization -- RooMinimizer::optimizeConst: activating const optimization\n",
      "[#1] INFO:Minization --  The following expressions have been identified as constant and will be precalculated and cached: (signal,bkg)\n",
      " **********\n",
      " **   46 **SET PRINT           1\n",
      " **********\n",
      " **********\n",
      " **   47 **SET NOGRAD\n",
      " **********\n",
      " PARAMETER DEFINITIONS:\n",
      "    NO.   NAME         VALUE      STEP SIZE      LIMITS\n",
      "     1 c0           5.00000e-01  1.00000e-01    0.00000e+00  1.00000e+00\n",
      " **********\n",
      " **   48 **SET ERR         0.5\n",
      " **********\n",
      " **********\n",
      " **   49 **SET PRINT           1\n",
      " **********\n",
      " **********\n",
      " **   50 **SET STR           1\n",
      " **********\n",
      " NOW USING STRATEGY  1: TRY TO BALANCE SPEED AGAINST RELIABILITY\n",
      " **********\n",
      " **   51 **MIGRAD         500           1\n",
      " **********\n",
      " FIRST CALL TO USER FUNCTION AT NEW START POINT, WITH IFLAG=4.\n",
      " START MIGRAD MINIMIZATION.  STRATEGY  1.  CONVERGENCE WHEN EDM .LT. 1.00e-03\n",
      " FCN=13429.1 FROM MIGRAD    STATUS=INITIATE        4 CALLS           5 TOTAL\n",
      "                     EDM= unknown      STRATEGY= 1      NO ERROR MATRIX       \n",
      "  EXT PARAMETER               CURRENT GUESS       STEP         FIRST   \n",
      "  NO.   NAME      VALUE            ERROR          SIZE      DERIVATIVE \n",
      "   1  c0           5.00000e-01   1.00000e-01   2.01358e-01   8.44100e+02\n",
      "                               ERR DEF= 0.5\n",
      " MIGRAD MINIMIZATION HAS CONVERGED.\n",
      " MIGRAD WILL VERIFY CONVERGENCE AND ERROR MATRIX.\n",
      " COVARIANCE MATRIX CALCULATED SUCCESSFULLY\n",
      " FCN=13053.1 FROM MIGRAD    STATUS=CONVERGED      19 CALLS          20 TOTAL\n",
      "                     EDM=1.99634e-09    STRATEGY= 1      ERROR MATRIX ACCURATE \n",
      "  EXT PARAMETER                                   STEP         FIRST   \n",
      "  NO.   NAME      VALUE            ERROR          SIZE      DERIVATIVE \n",
      "   1  c0           1.11587e-01   1.06595e-02   2.67319e-03   1.31951e-03\n",
      "                               ERR DEF= 0.5\n",
      " EXTERNAL ERROR MATRIX.    NDIM=  25    NPAR=  1    ERR DEF=0.5\n",
      "  1.137e-04 \n",
      " **********\n",
      " **   52 **SET ERR         0.5\n",
      " **********\n",
      " **********\n",
      " **   53 **SET PRINT           1\n",
      " **********\n",
      " **********\n",
      " **   54 **HESSE         500\n",
      " **********\n",
      " COVARIANCE MATRIX CALCULATED SUCCESSFULLY\n",
      " FCN=13053.1 FROM HESSE     STATUS=OK              5 CALLS          25 TOTAL\n",
      "                     EDM=1.99243e-09    STRATEGY= 1      ERROR MATRIX ACCURATE \n",
      "  EXT PARAMETER                                INTERNAL      INTERNAL  \n",
      "  NO.   NAME      VALUE            ERROR       STEP SIZE       VALUE   \n",
      "   1  c0           1.11587e-01   1.06595e-02   1.06928e-04  -8.89609e-01\n",
      "                               ERR DEF= 0.5\n",
      " EXTERNAL ERROR MATRIX.    NDIM=  25    NPAR=  1    ERR DEF=0.5\n",
      "  1.137e-04 \n",
      "[#1] INFO:Minization -- RooMinimizer::optimizeConst: deactivating const optimization\n"
     ]
    }
   ],
   "source": [
    "fit(hists, \"MET_met\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-clarity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
